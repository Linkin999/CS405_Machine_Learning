Fri Jan 12 00:00:29 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  | 00000000:0B:00.0 Off |                  N/A |
| 27%   31C    P8              26W / 250W |      2MiB / 11264MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Found 2975 train images
Found 500 val images
[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
File [0;32m<ipython-input-1-43643e604710>:2[0m
[1;32m      1[0m [38;5;66;03m# Instance of the model defined above.[39;00m
[0;32m----> 2[0m model [38;5;241m=[39m [43mR2U_Net[49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1160[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
[1;32m   1156[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1157[0m                     non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[1;32m   1158[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m, non_blocking)
[0;32m-> 1160[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

    [0;31m[... skipping similar frames: Module._apply at line 810 (2 times)][0m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:833[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    829[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
[1;32m    830[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
[1;32m    831[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
[1;32m    832[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 833[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
[1;32m    834[0m should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
[1;32m    835[0m [38;5;28;01mif[39;00m should_use_set_data:

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1158[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
[1;32m   1155[0m [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
[1;32m   1156[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1157[0m                 non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[0;32m-> 1158[0m [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m,[49m[43m [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[43m [49m[43mnon_blocking[49m[43m)[49m

[0;31mRuntimeError[0m: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [0;32mIn[1], line 1[0m
[0;32m----> 1[0m [43mget_ipython[49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mrun_line_magic[49m[43m([49m[38;5;124;43m'[39;49m[38;5;124;43mrun[39;49m[38;5;124;43m'[39;49m[43m,[49m[43m [49m[38;5;124;43m'[39;49m[38;5;124;43mCityscapes-R2UNET.ipynb[39;49m[38;5;124;43m'[39;49m[43m)[49m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2456[0m, in [0;36mInteractiveShell.run_line_magic[0;34m(self, magic_name, line, _stack_depth)[0m
[1;32m   2454[0m     kwargs[[38;5;124m'[39m[38;5;124mlocal_ns[39m[38;5;124m'[39m] [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mget_local_scope(stack_depth)
[1;32m   2455[0m [38;5;28;01mwith[39;00m [38;5;28mself[39m[38;5;241m.[39mbuiltin_trap:
[0;32m-> 2456[0m     result [38;5;241m=[39m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   2458[0m [38;5;66;03m# The code below prevents the output from being displayed[39;00m
[1;32m   2459[0m [38;5;66;03m# when using magics with decorator @output_can_be_silenced[39;00m
[1;32m   2460[0m [38;5;66;03m# when the last Python token in the expression is a ';'.[39;00m
[1;32m   2461[0m [38;5;28;01mif[39;00m [38;5;28mgetattr[39m(fn, magic[38;5;241m.[39mMAGIC_OUTPUT_CAN_BE_SILENCED, [38;5;28;01mFalse[39;00m):

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/IPython/core/magics/execution.py:737[0m, in [0;36mExecutionMagics.run[0;34m(self, parameter_s, runner, file_finder)[0m
[1;32m    735[0m     [38;5;28;01mwith[39;00m preserve_keys([38;5;28mself[39m[38;5;241m.[39mshell[38;5;241m.[39muser_ns, [38;5;124m'[39m[38;5;124m__file__[39m[38;5;124m'[39m):
[1;32m    736[0m         [38;5;28mself[39m[38;5;241m.[39mshell[38;5;241m.[39muser_ns[[38;5;124m'[39m[38;5;124m__file__[39m[38;5;124m'[39m] [38;5;241m=[39m filename
[0;32m--> 737[0m         [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mshell[49m[38;5;241;43m.[39;49m[43msafe_execfile_ipy[49m[43m([49m[43mfilename[49m[43m,[49m[43m [49m[43mraise_exceptions[49m[38;5;241;43m=[39;49m[38;5;28;43;01mTrue[39;49;00m[43m)[49m
[1;32m    738[0m     [38;5;28;01mreturn[39;00m
[1;32m    740[0m [38;5;66;03m# Control the response to exit() calls made by the script being run[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2978[0m, in [0;36mInteractiveShell.safe_execfile_ipy[0;34m(self, fname, shell_futures, raise_exceptions)[0m
[1;32m   2976[0m result [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mrun_cell(cell, silent[38;5;241m=[39m[38;5;28;01mTrue[39;00m, shell_futures[38;5;241m=[39mshell_futures)
[1;32m   2977[0m [38;5;28;01mif[39;00m raise_exceptions:
[0;32m-> 2978[0m     [43mresult[49m[38;5;241;43m.[39;49m[43mraise_error[49m[43m([49m[43m)[49m
[1;32m   2979[0m [38;5;28;01melif[39;00m [38;5;129;01mnot[39;00m result[38;5;241m.[39msuccess:
[1;32m   2980[0m     [38;5;28;01mbreak[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:294[0m, in [0;36mExecutionResult.raise_error[0;34m(self)[0m
[1;32m    292[0m     [38;5;28;01mraise[39;00m [38;5;28mself[39m[38;5;241m.[39merror_before_exec
[1;32m    293[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39merror_in_exec [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m:
[0;32m--> 294[0m     [38;5;28;01mraise[39;00m [38;5;28mself[39m[38;5;241m.[39merror_in_exec

    [0;31m[... skipping hidden 1 frame][0m

File [0;32m<ipython-input-1-43643e604710>:2[0m
[1;32m      1[0m [38;5;66;03m# Instance of the model defined above.[39;00m
[0;32m----> 2[0m model [38;5;241m=[39m [43mR2U_Net[49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1160[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
[1;32m   1156[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1157[0m                     non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[1;32m   1158[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m, non_blocking)
[0;32m-> 1160[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

    [0;31m[... skipping similar frames: Module._apply at line 810 (2 times)][0m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    808[0m [38;5;28;01mif[39;00m recurse:
[1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:833[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
[1;32m    829[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
[1;32m    830[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
[1;32m    831[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
[1;32m    832[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 833[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
[1;32m    834[0m should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
[1;32m    835[0m [38;5;28;01mif[39;00m should_use_set_data:

File [0;32m~/software/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1158[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
[1;32m   1155[0m [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
[1;32m   1156[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1157[0m                 non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[0;32m-> 1158[0m [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m,[49m[43m [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[43m [49m[43mnon_blocking[49m[43m)[49m

[0;31mRuntimeError[0m: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

