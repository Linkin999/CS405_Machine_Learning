{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 tutorial for Machine Learning <br >Neural NetWork & Pytorch\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- Install the pytorch neural network framework for your computer.\n",
    "- Learn to use pytorch.\n",
    "- Implement a simple neural network using pytorch\n",
    "- Complete the LAB assignment.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of  machine learning and are at the heart of  deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n",
    "\n",
    "![img](images/1hkYlTODpjJgo32DoCOWN5w.png)\n",
    "\n",
    "The above neural could be represented as:\n",
    "$$y=f(b+\\sum_{i=1}^{n}w_ix_i)=f(w^Tx)$$\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing `an input layer`, `one or more hidden layers`, and `an output layer`. Each node, or artificial neuron, connects to another and has an associated `weight` and `threshold`. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "<img src=\"images/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\" alt=\"Visual diagram of an input layer, hidden layers, and an output layer of a feedforward neural network \" style=\"zoom:40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The role of neural networks in Machine learning\n",
    "+ Supervise machine learning process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Supervise machine learning process ](images/Supervise-machine-learning-process.png)\n",
    "\n",
    "  In the figure above, the hardest part is how to obtain valid feature vectors. This is a technique called **feature engineering**.\n",
    "\n",
    "+ **The Importance of Feature Engineering:**\n",
    "\n",
    "  + Preprocessing and feature extraction determine the upper bound of the model\n",
    "  + The algorithm and parameter selection approach this upper bound.\n",
    "\n",
    "+ **Traditional feature extraction methods:**\n",
    "\n",
    "  <img src=\"images/image-20221020212446504.png\" alt=\"image-20221020212446504 \" style=\"zoom:60%;\" />\n",
    "\n",
    "+ **Neural networks automatically extract features**\n",
    "  \n",
    "  <img src=\"images/image-20221020212805853.png\" alt=\"image-20221020212805853 \" style=\"zoom:60%;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### Install Pytorch\n",
    "Please refer to **Installing PyTorch on Windows 10.md** for this section:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with linear regression\n",
    "#### Linear regression\n",
    "Do you remember the general form of linear regression model's prediction?\n",
    "$$\\hat{y}=h_{\\theta }(x)=\\theta _{0}+\\theta _{1}x_{1}+\\theta _{2}x_{2}+...+\\theta _{n}x_{n}=\\theta ^{T}\\cdot x$$\n",
    "\n",
    "<center>\n",
    "    <img src='images/fit-linreg.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Fitting a linear regression model to one-dimensional data\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Linear regression is a single-layer neural network, We used this simple network to learn how to use pytorch.\n",
    "<center>\n",
    "    <img src='images/singleneuron.svg' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "        Linear regression is a single-layer neural network \n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up: [numpy](https://numpy.org/learn/)\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e31c5fb610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "plt.scatter(x,y,c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 620.2979910404363\n",
      "199 439.29197876720593\n",
      "299 311.9627985310707\n",
      "399 222.35969722621437\n",
      "499 159.28358319183377\n",
      "599 114.86697098268226\n",
      "699 83.58048845262378\n",
      "799 61.53643702057845\n",
      "899 46.000323813060405\n",
      "999 35.048085408708715\n",
      "1099 27.325431833916372\n",
      "1199 21.878809006947712\n",
      "1299 18.03661420249434\n",
      "1399 15.325690213776586\n",
      "1499 13.412597458474139\n",
      "1599 12.062296532171978\n",
      "1699 11.109069496719773\n",
      "1799 10.436047843510485\n",
      "1899 9.960795221654482\n",
      "1999 9.625151183481577\n",
      "Result: y = 0.029738245282547114 + 0.8609397264898517 x + -0.0051303421577768545 x^2 + -0.09392764364233594 x^3\n"
     ]
    }
   ],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors \n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 735.03271484375\n",
      "199 513.287841796875\n",
      "299 359.5666809082031\n",
      "399 252.8993377685547\n",
      "499 178.8137969970703\n",
      "599 127.31133270263672\n",
      "699 91.47676849365234\n",
      "799 66.52253723144531\n",
      "899 49.1308479309082\n",
      "999 37.000328063964844\n",
      "1099 28.532962799072266\n",
      "1199 22.61827850341797\n",
      "1299 18.483842849731445\n",
      "1399 15.59187126159668\n",
      "1499 13.567693710327148\n",
      "1599 12.15005874633789\n",
      "1699 11.156631469726562\n",
      "1799 10.460077285766602\n",
      "1899 9.97143840789795\n",
      "1999 9.628480911254883\n",
      "Result: y = 0.028652291744947433 + 0.8481237888336182 x + -0.004942995961755514 x^2 + -0.09210468828678131 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: Tensors and autograd\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. \n",
    "\n",
    "When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1998.1025390625\n",
      "199 1335.93212890625\n",
      "299 894.65380859375\n",
      "399 600.4371337890625\n",
      "499 404.1725158691406\n",
      "599 273.18048095703125\n",
      "699 185.70404052734375\n",
      "799 127.25296020507812\n",
      "899 88.17292022705078\n",
      "999 62.0271110534668\n",
      "1099 44.52292251586914\n",
      "1199 32.796119689941406\n",
      "1299 24.934131622314453\n",
      "1399 19.6590576171875\n",
      "1499 16.116939544677734\n",
      "1599 13.736551284790039\n",
      "1699 12.13553237915039\n",
      "1799 11.057717323303223\n",
      "1899 10.3314847946167\n",
      "1999 9.841692924499512\n",
      "Result: y = -0.019899722188711166 + 0.8819229602813721 x + 0.0034330335911363363 x^2 + -0.09691232442855835 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.data.item())\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_a = grad_y_pred.sum()\n",
    "    # grad_b = (grad_y_pred * x).sum()\n",
    "    # grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    # grad_d = (grad_y_pred * x ** 3).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a.data -= learning_rate * a.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    c.data -= learning_rate * c.grad.data\n",
    "    d.data -= learning_rate * d.grad.data\n",
    "\n",
    "    a.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "    c.grad.data.zero_()\n",
    "    d.grad.data.zero_()\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.data.item()} + {b.data.item()} x + {c.data.item()} x^2 + {d.data.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch: [nn module](https://pytorch.org/docs/stable/nn.html)\n",
    "we use the nn package to implement our polynomial model network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1168.6158447265625\n",
      "199 785.5199584960938\n",
      "299 529.3370971679688\n",
      "399 357.9115295410156\n",
      "499 243.1237335205078\n",
      "599 166.20648193359375\n",
      "699 114.62760925292969\n",
      "799 80.01326751708984\n",
      "899 56.76527786254883\n",
      "999 41.138328552246094\n",
      "1099 30.624998092651367\n",
      "1199 23.54580307006836\n",
      "1299 18.774581909179688\n",
      "1399 15.555818557739258\n",
      "1499 13.382280349731445\n",
      "1599 11.913124084472656\n",
      "1699 10.919078826904297\n",
      "1799 10.245747566223145\n",
      "1899 9.789178848266602\n",
      "1999 9.479296684265137\n",
      "Result: y = -0.018197132274508476 + 0.8381290435791016 x + 0.00313930818811059 x^2 + -0.09068302810192108 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define the class\n",
    "  \n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward()` method.\n",
    "The `forward()` method is in charge of conducting the **forward propagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()       \n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.flatten = nn.Flatten(0, 1)\n",
    "#       self.model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(3, 1),\n",
    "#     torch.nn.Flatten(0, 1)\n",
    "# )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(self.linear(x))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 352.55938720703125\n",
      "199 237.2746124267578\n",
      "299 160.7042236328125\n",
      "399 109.83244323730469\n",
      "499 76.02391815185547\n",
      "599 53.54804229736328\n",
      "699 38.60101318359375\n",
      "799 28.657146453857422\n",
      "899 22.039304733276367\n",
      "999 17.633155822753906\n",
      "1099 14.6983003616333\n",
      "1199 12.742547035217285\n",
      "1299 11.438608169555664\n",
      "1399 10.568862915039062\n",
      "1499 9.988395690917969\n",
      "1599 9.600798606872559\n",
      "1699 9.341819763183594\n",
      "1799 9.168670654296875\n",
      "1899 9.052848815917969\n",
      "1999 8.975303649902344\n",
      "Result: y = 0.006259738467633724 + 0.8459528088569641 x + -0.0010799093870446086 x^2 + -0.09179588407278061 x^3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model.linear\n",
    "\n",
    "\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with `torch.no_grad()`. This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we will optimize the model using the `RMSprop` algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 8744.5234375\n",
      "199 3333.590576171875\n",
      "299 1846.7325439453125\n",
      "399 1509.55322265625\n",
      "499 1318.0645751953125\n",
      "599 1120.4249267578125\n",
      "699 929.8475952148438\n",
      "799 756.7149047851562\n",
      "899 603.3914794921875\n",
      "999 469.7457275390625\n",
      "1099 355.2457275390625\n",
      "1199 259.1714172363281\n",
      "1299 180.72760009765625\n",
      "1399 119.0359115600586\n",
      "1499 73.24058532714844\n",
      "1599 41.53657913208008\n",
      "1699 22.31664276123047\n",
      "1799 12.83145523071289\n",
      "1899 9.596765518188477\n",
      "1999 8.952305793762207\n",
      "Result: y = -0.0004920574720017612 + 0.8508240580558777 x + -0.0004922038642689586 x^2 + -0.09195192903280258 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "model.requires_grad_()\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(),lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=1e-6,momentum=0.9)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted y by passing x to the model. \n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "   \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())    \n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. \n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning pytorch with logistic regression\n",
    "If we use a single-layer network for classification, this is known as a logistic regression.\n",
    "\n",
    "\n",
    " We need to add the sigmoid function to the output of the linear regression.\n",
    "<center>\n",
    "    <img src='images/Center.png' style=\"zoom:100%;\"/>\n",
    "    <br>\n",
    "    <div style=\"\">\n",
    "       Perceptron\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Let us define the number of epochs and the learning rate we want our model for training. As the data is a binary  classification, we will use **Binary Cross Entropy** as the **loss function** used to optimize the model using an `SGD optimizer`.\n",
    "\n",
    "<font size=5 color='red'>Please complete this part of the code!!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### Perceptron-The basic unit of neural network\n",
    "A simple model of a biological neuron in an artificial neural network is known as Perceptron. it is the primary step to learn Machine Learning and Deep Learning technologies.\n",
    "\n",
    "we can consider it as a single-layer neural network with four main parameters, i.e., `input values`, `weights and Bias`, `net sum`, and an `activation function`.\n",
    "\n",
    "![Perceptron in Machine Learning](images/perceptron-in-machine-learning2.png)\n",
    "\n",
    "- **Input Nodes or Input Layer:**\n",
    "\n",
    "This is the primary component of Perceptron which accepts the initial data into the system for further processing.\n",
    "\n",
    "- **Wight and Bias:**\n",
    "\n",
    "Weight parameter represents the strength of the connection between units.  Bias can be considered as the intercept in a linear equation.\n",
    "\n",
    "- **Activation Function:**\n",
    "\n",
    "These are the final and important components that help to determine whether the neuron will fire or not. The activation function of perceptron is `sign function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron(MLP)\n",
    "A neuron is a mathematical model of the behaviour of a single neuron in a biological nervous system.\n",
    "\n",
    "A single neuron can solve some simple tasks, but the power of neural networks comes when many of them are arranged in layers and connected in a network architecture.\n",
    "\n",
    "<img src=\"images/multilayer-perceptron-1.png\" alt=\"multilayer-perceptron-1 \" style=\"zoom:40%;\" />\n",
    "\n",
    "\n",
    "**A Multilayered Perceptron is a Neural Network**. A neural network having more than 3 hidden layers is called a **Deep Neural Network**.\n",
    "\n",
    "In this lab, Multilayer Perceptron and Neural Network  mean the same thing.\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Various activation functions that can be used with Perceptron.\n",
    "\n",
    "![Perceptron_36.](images/Perceptron_36.jpg)\n",
    "\n",
    "<font color=\"red\">Neural network without activation functions are simply linear regression model</font>. The activation makes the input capable of learning and performing more complex tasks.\n",
    "\n",
    "![image-20221023014216170](images/image-20221023014216170.png)\n",
    "\n",
    "Therefore, when we write the neural network framework, the neurons in each hidden layer are most of the time **followed by an activation function**.\n",
    "\n",
    "I recommend that you use the relu function as you build your neural network framework.\n",
    "\n",
    "![image-20221023015025204](images/image-20221023015025204.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for MLP\n",
    "This Example uses dataset digit123.csv , which has 36 columns, and the last column is the dependent variable. We use this dataset to familiarize ourselves with MLP and solve the multi-classification problem.\n",
    "\n",
    "\n",
    "**Note that the values of the dependent variable are 1,2,3, and label coding is required.**\n",
    "#### MLP Model \n",
    "\n",
    "+ step 1 load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  27  28  29  30  31  32  33  \\\n",
       "0   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "1   0   0   0   1   0   0   0   0   1   1  ...   1   0   0   0   0   0   1   \n",
       "2   0   0   1   1   0   0   0   0   0   1  ...   1   0   0   0   0   0   1   \n",
       "3   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "4   0   0   0   1   0   0   0   0   0   1  ...   1   0   0   0   0   1   1   \n",
       "\n",
       "   34  35  36  \n",
       "0   0   0   1  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   1  \n",
       "4   1   0   1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# ============================ step 1/6 load datasets ============================\n",
    "df = pd.read_csv(\"datasets/digit123.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32\n",
       "2    32\n",
       "3    32\n",
       "Name: 36, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 36)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[36]\n",
    "y.replace((1, 2, 3),(0, 1, 2),inplace=True)\n",
    "X = df.drop(36, axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting for X and Y variables:\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Splitting dataset into 80% Training and 20% Testing Data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, random_state =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "#Converting them to tensors as PyTorch works on, we will use the torch.from_numpy() method:\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 2 Define a MLP subclass of nn. Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a MLP subclass of nn. Module.\n",
    "# ============================ step 2/6 define model ============================\n",
    "import  torch \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(self.flatten(input))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 3 Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.001 \n",
    "# Create the model\n",
    "# ============================ step 3/6 Create model ============================\n",
    "models = MLP(X_train.shape[1],X_train.shape[1]//2,y_train.unique().size()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 4 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 Loss function ============================\n",
    "criterions = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 5 The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ step 4/6 The optimizer ============================\n",
    "optimizers = torch.optim.SGD(models.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 6 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss = 1.1092\n",
      "epoch: 40, loss = 1.1060\n",
      "epoch: 60, loss = 1.1027\n",
      "epoch: 80, loss = 1.0995\n",
      "epoch: 100, loss = 1.0963\n",
      "epoch: 120, loss = 1.0931\n",
      "epoch: 140, loss = 1.0899\n",
      "epoch: 160, loss = 1.0867\n",
      "epoch: 180, loss = 1.0836\n",
      "epoch: 200, loss = 1.0804\n",
      "epoch: 220, loss = 1.0773\n",
      "epoch: 240, loss = 1.0742\n",
      "epoch: 260, loss = 1.0711\n",
      "epoch: 280, loss = 1.0681\n",
      "epoch: 300, loss = 1.0650\n",
      "epoch: 320, loss = 1.0620\n",
      "epoch: 340, loss = 1.0590\n",
      "epoch: 360, loss = 1.0559\n",
      "epoch: 380, loss = 1.0529\n",
      "epoch: 400, loss = 1.0499\n",
      "epoch: 420, loss = 1.0468\n",
      "epoch: 440, loss = 1.0438\n",
      "epoch: 460, loss = 1.0408\n",
      "epoch: 480, loss = 1.0377\n",
      "epoch: 500, loss = 1.0347\n",
      "epoch: 520, loss = 1.0317\n",
      "epoch: 540, loss = 1.0287\n",
      "epoch: 560, loss = 1.0257\n",
      "epoch: 580, loss = 1.0226\n",
      "epoch: 600, loss = 1.0196\n",
      "epoch: 620, loss = 1.0165\n",
      "epoch: 640, loss = 1.0135\n",
      "epoch: 660, loss = 1.0104\n",
      "epoch: 680, loss = 1.0073\n",
      "epoch: 700, loss = 1.0042\n",
      "epoch: 720, loss = 1.0011\n",
      "epoch: 740, loss = 0.9980\n",
      "epoch: 760, loss = 0.9948\n",
      "epoch: 780, loss = 0.9917\n",
      "epoch: 800, loss = 0.9886\n",
      "epoch: 820, loss = 0.9854\n",
      "epoch: 840, loss = 0.9822\n",
      "epoch: 860, loss = 0.9790\n",
      "epoch: 880, loss = 0.9757\n",
      "epoch: 900, loss = 0.9724\n",
      "epoch: 920, loss = 0.9692\n",
      "epoch: 940, loss = 0.9659\n",
      "epoch: 960, loss = 0.9627\n",
      "epoch: 980, loss = 0.9593\n",
      "epoch: 1000, loss = 0.9560\n",
      "epoch: 1020, loss = 0.9526\n",
      "epoch: 1040, loss = 0.9493\n",
      "epoch: 1060, loss = 0.9459\n",
      "epoch: 1080, loss = 0.9425\n",
      "epoch: 1100, loss = 0.9390\n",
      "epoch: 1120, loss = 0.9356\n",
      "epoch: 1140, loss = 0.9321\n",
      "epoch: 1160, loss = 0.9287\n",
      "epoch: 1180, loss = 0.9252\n",
      "epoch: 1200, loss = 0.9216\n",
      "epoch: 1220, loss = 0.9181\n",
      "epoch: 1240, loss = 0.9145\n",
      "epoch: 1260, loss = 0.9110\n",
      "epoch: 1280, loss = 0.9074\n",
      "epoch: 1300, loss = 0.9038\n",
      "epoch: 1320, loss = 0.9001\n",
      "epoch: 1340, loss = 0.8965\n",
      "epoch: 1360, loss = 0.8928\n",
      "epoch: 1380, loss = 0.8891\n",
      "epoch: 1400, loss = 0.8853\n",
      "epoch: 1420, loss = 0.8815\n",
      "epoch: 1440, loss = 0.8777\n",
      "epoch: 1460, loss = 0.8739\n",
      "epoch: 1480, loss = 0.8701\n",
      "epoch: 1500, loss = 0.8663\n",
      "epoch: 1520, loss = 0.8624\n",
      "epoch: 1540, loss = 0.8586\n",
      "epoch: 1560, loss = 0.8547\n",
      "epoch: 1580, loss = 0.8508\n",
      "epoch: 1600, loss = 0.8469\n",
      "epoch: 1620, loss = 0.8430\n",
      "epoch: 1640, loss = 0.8391\n",
      "epoch: 1660, loss = 0.8352\n",
      "epoch: 1680, loss = 0.8312\n",
      "epoch: 1700, loss = 0.8272\n",
      "epoch: 1720, loss = 0.8232\n",
      "epoch: 1740, loss = 0.8192\n",
      "epoch: 1760, loss = 0.8151\n",
      "epoch: 1780, loss = 0.8110\n",
      "epoch: 1800, loss = 0.8069\n",
      "epoch: 1820, loss = 0.8027\n",
      "epoch: 1840, loss = 0.7986\n",
      "epoch: 1860, loss = 0.7944\n",
      "epoch: 1880, loss = 0.7902\n",
      "epoch: 1900, loss = 0.7860\n",
      "epoch: 1920, loss = 0.7818\n",
      "epoch: 1940, loss = 0.7776\n",
      "epoch: 1960, loss = 0.7733\n",
      "epoch: 1980, loss = 0.7691\n",
      "epoch: 2000, loss = 0.7648\n",
      "epoch: 2020, loss = 0.7605\n",
      "epoch: 2040, loss = 0.7562\n",
      "epoch: 2060, loss = 0.7519\n",
      "epoch: 2080, loss = 0.7475\n",
      "epoch: 2100, loss = 0.7432\n",
      "epoch: 2120, loss = 0.7388\n",
      "epoch: 2140, loss = 0.7344\n",
      "epoch: 2160, loss = 0.7301\n",
      "epoch: 2180, loss = 0.7257\n",
      "epoch: 2200, loss = 0.7213\n",
      "epoch: 2220, loss = 0.7169\n",
      "epoch: 2240, loss = 0.7125\n",
      "epoch: 2260, loss = 0.7081\n",
      "epoch: 2280, loss = 0.7037\n",
      "epoch: 2300, loss = 0.6993\n",
      "epoch: 2320, loss = 0.6948\n",
      "epoch: 2340, loss = 0.6904\n",
      "epoch: 2360, loss = 0.6860\n",
      "epoch: 2380, loss = 0.6816\n",
      "epoch: 2400, loss = 0.6771\n",
      "epoch: 2420, loss = 0.6727\n",
      "epoch: 2440, loss = 0.6683\n",
      "epoch: 2460, loss = 0.6639\n",
      "epoch: 2480, loss = 0.6594\n",
      "epoch: 2500, loss = 0.6550\n",
      "epoch: 2520, loss = 0.6506\n",
      "epoch: 2540, loss = 0.6462\n",
      "epoch: 2560, loss = 0.6417\n",
      "epoch: 2580, loss = 0.6373\n",
      "epoch: 2600, loss = 0.6329\n",
      "epoch: 2620, loss = 0.6284\n",
      "epoch: 2640, loss = 0.6240\n",
      "epoch: 2660, loss = 0.6196\n",
      "epoch: 2680, loss = 0.6152\n",
      "epoch: 2700, loss = 0.6107\n",
      "epoch: 2720, loss = 0.6063\n",
      "epoch: 2740, loss = 0.6019\n",
      "epoch: 2760, loss = 0.5975\n",
      "epoch: 2780, loss = 0.5932\n",
      "epoch: 2800, loss = 0.5888\n",
      "epoch: 2820, loss = 0.5844\n",
      "epoch: 2840, loss = 0.5800\n",
      "epoch: 2860, loss = 0.5757\n",
      "epoch: 2880, loss = 0.5713\n",
      "epoch: 2900, loss = 0.5670\n",
      "epoch: 2920, loss = 0.5627\n",
      "epoch: 2940, loss = 0.5583\n",
      "epoch: 2960, loss = 0.5540\n",
      "epoch: 2980, loss = 0.5498\n",
      "epoch: 3000, loss = 0.5455\n",
      "epoch: 3020, loss = 0.5412\n",
      "epoch: 3040, loss = 0.5370\n",
      "epoch: 3060, loss = 0.5327\n",
      "epoch: 3080, loss = 0.5285\n",
      "epoch: 3100, loss = 0.5243\n",
      "epoch: 3120, loss = 0.5201\n",
      "epoch: 3140, loss = 0.5159\n",
      "epoch: 3160, loss = 0.5117\n",
      "epoch: 3180, loss = 0.5076\n",
      "epoch: 3200, loss = 0.5035\n",
      "epoch: 3220, loss = 0.4994\n",
      "epoch: 3240, loss = 0.4953\n",
      "epoch: 3260, loss = 0.4912\n",
      "epoch: 3280, loss = 0.4872\n",
      "epoch: 3300, loss = 0.4832\n",
      "epoch: 3320, loss = 0.4791\n",
      "epoch: 3340, loss = 0.4752\n",
      "epoch: 3360, loss = 0.4712\n",
      "epoch: 3380, loss = 0.4672\n",
      "epoch: 3400, loss = 0.4633\n",
      "epoch: 3420, loss = 0.4594\n",
      "epoch: 3440, loss = 0.4555\n",
      "epoch: 3460, loss = 0.4517\n",
      "epoch: 3480, loss = 0.4479\n",
      "epoch: 3500, loss = 0.4441\n",
      "epoch: 3520, loss = 0.4403\n",
      "epoch: 3540, loss = 0.4366\n",
      "epoch: 3560, loss = 0.4329\n",
      "epoch: 3580, loss = 0.4292\n",
      "epoch: 3600, loss = 0.4255\n",
      "epoch: 3620, loss = 0.4219\n",
      "epoch: 3640, loss = 0.4182\n",
      "epoch: 3660, loss = 0.4146\n",
      "epoch: 3680, loss = 0.4111\n",
      "epoch: 3700, loss = 0.4076\n",
      "epoch: 3720, loss = 0.4041\n",
      "epoch: 3740, loss = 0.4006\n",
      "epoch: 3760, loss = 0.3972\n",
      "epoch: 3780, loss = 0.3937\n",
      "epoch: 3800, loss = 0.3904\n",
      "epoch: 3820, loss = 0.3870\n",
      "epoch: 3840, loss = 0.3837\n",
      "epoch: 3860, loss = 0.3804\n",
      "epoch: 3880, loss = 0.3771\n",
      "epoch: 3900, loss = 0.3739\n",
      "epoch: 3920, loss = 0.3707\n",
      "epoch: 3940, loss = 0.3675\n",
      "epoch: 3960, loss = 0.3643\n",
      "epoch: 3980, loss = 0.3612\n",
      "epoch: 4000, loss = 0.3581\n",
      "epoch: 4020, loss = 0.3551\n",
      "epoch: 4040, loss = 0.3520\n",
      "epoch: 4060, loss = 0.3490\n",
      "epoch: 4080, loss = 0.3460\n",
      "epoch: 4100, loss = 0.3431\n",
      "epoch: 4120, loss = 0.3402\n",
      "epoch: 4140, loss = 0.3373\n",
      "epoch: 4160, loss = 0.3344\n",
      "epoch: 4180, loss = 0.3316\n",
      "epoch: 4200, loss = 0.3288\n",
      "epoch: 4220, loss = 0.3260\n",
      "epoch: 4240, loss = 0.3233\n",
      "epoch: 4260, loss = 0.3205\n",
      "epoch: 4280, loss = 0.3179\n",
      "epoch: 4300, loss = 0.3152\n",
      "epoch: 4320, loss = 0.3126\n",
      "epoch: 4340, loss = 0.3100\n",
      "epoch: 4360, loss = 0.3074\n",
      "epoch: 4380, loss = 0.3048\n",
      "epoch: 4400, loss = 0.3023\n",
      "epoch: 4420, loss = 0.2998\n",
      "epoch: 4440, loss = 0.2974\n",
      "epoch: 4460, loss = 0.2949\n",
      "epoch: 4480, loss = 0.2925\n",
      "epoch: 4500, loss = 0.2901\n",
      "epoch: 4520, loss = 0.2878\n",
      "epoch: 4540, loss = 0.2854\n",
      "epoch: 4560, loss = 0.2831\n",
      "epoch: 4580, loss = 0.2808\n",
      "epoch: 4600, loss = 0.2786\n",
      "epoch: 4620, loss = 0.2763\n",
      "epoch: 4640, loss = 0.2741\n",
      "epoch: 4660, loss = 0.2719\n",
      "epoch: 4680, loss = 0.2698\n",
      "epoch: 4700, loss = 0.2677\n",
      "epoch: 4720, loss = 0.2655\n",
      "epoch: 4740, loss = 0.2635\n",
      "epoch: 4760, loss = 0.2614\n",
      "epoch: 4780, loss = 0.2594\n",
      "epoch: 4800, loss = 0.2574\n",
      "epoch: 4820, loss = 0.2554\n",
      "epoch: 4840, loss = 0.2534\n",
      "epoch: 4860, loss = 0.2514\n",
      "epoch: 4880, loss = 0.2495\n",
      "epoch: 4900, loss = 0.2476\n",
      "epoch: 4920, loss = 0.2457\n",
      "epoch: 4940, loss = 0.2439\n",
      "epoch: 4960, loss = 0.2420\n",
      "epoch: 4980, loss = 0.2402\n",
      "epoch: 5000, loss = 0.2384\n",
      "epoch: 5020, loss = 0.2367\n",
      "epoch: 5040, loss = 0.2349\n",
      "epoch: 5060, loss = 0.2332\n",
      "epoch: 5080, loss = 0.2315\n",
      "epoch: 5100, loss = 0.2298\n",
      "epoch: 5120, loss = 0.2281\n",
      "epoch: 5140, loss = 0.2264\n",
      "epoch: 5160, loss = 0.2248\n",
      "epoch: 5180, loss = 0.2232\n",
      "epoch: 5200, loss = 0.2216\n",
      "epoch: 5220, loss = 0.2200\n",
      "epoch: 5240, loss = 0.2185\n",
      "epoch: 5260, loss = 0.2169\n",
      "epoch: 5280, loss = 0.2154\n",
      "epoch: 5300, loss = 0.2139\n",
      "epoch: 5320, loss = 0.2124\n",
      "epoch: 5340, loss = 0.2109\n",
      "epoch: 5360, loss = 0.2095\n",
      "epoch: 5380, loss = 0.2080\n",
      "epoch: 5400, loss = 0.2066\n",
      "epoch: 5420, loss = 0.2052\n",
      "epoch: 5440, loss = 0.2038\n",
      "epoch: 5460, loss = 0.2024\n",
      "epoch: 5480, loss = 0.2011\n",
      "epoch: 5500, loss = 0.1997\n",
      "epoch: 5520, loss = 0.1984\n",
      "epoch: 5540, loss = 0.1971\n",
      "epoch: 5560, loss = 0.1958\n",
      "epoch: 5580, loss = 0.1945\n",
      "epoch: 5600, loss = 0.1932\n",
      "epoch: 5620, loss = 0.1920\n",
      "epoch: 5640, loss = 0.1908\n",
      "epoch: 5660, loss = 0.1895\n",
      "epoch: 5680, loss = 0.1883\n",
      "epoch: 5700, loss = 0.1871\n",
      "epoch: 5720, loss = 0.1860\n",
      "epoch: 5740, loss = 0.1848\n",
      "epoch: 5760, loss = 0.1836\n",
      "epoch: 5780, loss = 0.1825\n",
      "epoch: 5800, loss = 0.1814\n",
      "epoch: 5820, loss = 0.1803\n",
      "epoch: 5840, loss = 0.1792\n",
      "epoch: 5860, loss = 0.1781\n",
      "epoch: 5880, loss = 0.1770\n",
      "epoch: 5900, loss = 0.1759\n",
      "epoch: 5920, loss = 0.1749\n",
      "epoch: 5940, loss = 0.1738\n",
      "epoch: 5960, loss = 0.1728\n",
      "epoch: 5980, loss = 0.1718\n",
      "epoch: 6000, loss = 0.1708\n",
      "epoch: 6020, loss = 0.1698\n",
      "epoch: 6040, loss = 0.1688\n",
      "epoch: 6060, loss = 0.1678\n",
      "epoch: 6080, loss = 0.1669\n",
      "epoch: 6100, loss = 0.1659\n",
      "epoch: 6120, loss = 0.1650\n",
      "epoch: 6140, loss = 0.1640\n",
      "epoch: 6160, loss = 0.1631\n",
      "epoch: 6180, loss = 0.1622\n",
      "epoch: 6200, loss = 0.1613\n",
      "epoch: 6220, loss = 0.1604\n",
      "epoch: 6240, loss = 0.1595\n",
      "epoch: 6260, loss = 0.1587\n",
      "epoch: 6280, loss = 0.1578\n",
      "epoch: 6300, loss = 0.1569\n",
      "epoch: 6320, loss = 0.1561\n",
      "epoch: 6340, loss = 0.1552\n",
      "epoch: 6360, loss = 0.1544\n",
      "epoch: 6380, loss = 0.1536\n",
      "epoch: 6400, loss = 0.1528\n",
      "epoch: 6420, loss = 0.1520\n",
      "epoch: 6440, loss = 0.1512\n",
      "epoch: 6460, loss = 0.1504\n",
      "epoch: 6480, loss = 0.1496\n",
      "epoch: 6500, loss = 0.1489\n",
      "epoch: 6520, loss = 0.1481\n",
      "epoch: 6540, loss = 0.1473\n",
      "epoch: 6560, loss = 0.1466\n",
      "epoch: 6580, loss = 0.1458\n",
      "epoch: 6600, loss = 0.1451\n",
      "epoch: 6620, loss = 0.1444\n",
      "epoch: 6640, loss = 0.1437\n",
      "epoch: 6660, loss = 0.1430\n",
      "epoch: 6680, loss = 0.1423\n",
      "epoch: 6700, loss = 0.1416\n",
      "epoch: 6720, loss = 0.1409\n",
      "epoch: 6740, loss = 0.1402\n",
      "epoch: 6760, loss = 0.1395\n",
      "epoch: 6780, loss = 0.1388\n",
      "epoch: 6800, loss = 0.1382\n",
      "epoch: 6820, loss = 0.1375\n",
      "epoch: 6840, loss = 0.1369\n",
      "epoch: 6860, loss = 0.1362\n",
      "epoch: 6880, loss = 0.1356\n",
      "epoch: 6900, loss = 0.1350\n",
      "epoch: 6920, loss = 0.1343\n",
      "epoch: 6940, loss = 0.1337\n",
      "epoch: 6960, loss = 0.1331\n",
      "epoch: 6980, loss = 0.1325\n",
      "epoch: 7000, loss = 0.1319\n",
      "epoch: 7020, loss = 0.1313\n",
      "epoch: 7040, loss = 0.1307\n",
      "epoch: 7060, loss = 0.1301\n",
      "epoch: 7080, loss = 0.1295\n",
      "epoch: 7100, loss = 0.1290\n",
      "epoch: 7120, loss = 0.1284\n",
      "epoch: 7140, loss = 0.1278\n",
      "epoch: 7160, loss = 0.1273\n",
      "epoch: 7180, loss = 0.1267\n",
      "epoch: 7200, loss = 0.1262\n",
      "epoch: 7220, loss = 0.1256\n",
      "epoch: 7240, loss = 0.1251\n",
      "epoch: 7260, loss = 0.1245\n",
      "epoch: 7280, loss = 0.1240\n",
      "epoch: 7300, loss = 0.1235\n",
      "epoch: 7320, loss = 0.1230\n",
      "epoch: 7340, loss = 0.1225\n",
      "epoch: 7360, loss = 0.1220\n",
      "epoch: 7380, loss = 0.1215\n",
      "epoch: 7400, loss = 0.1210\n",
      "epoch: 7420, loss = 0.1205\n",
      "epoch: 7440, loss = 0.1200\n",
      "epoch: 7460, loss = 0.1195\n",
      "epoch: 7480, loss = 0.1190\n",
      "epoch: 7500, loss = 0.1185\n",
      "epoch: 7520, loss = 0.1180\n",
      "epoch: 7540, loss = 0.1176\n",
      "epoch: 7560, loss = 0.1171\n",
      "epoch: 7580, loss = 0.1166\n",
      "epoch: 7600, loss = 0.1162\n",
      "epoch: 7620, loss = 0.1157\n",
      "epoch: 7640, loss = 0.1153\n",
      "epoch: 7660, loss = 0.1148\n",
      "epoch: 7680, loss = 0.1144\n",
      "epoch: 7700, loss = 0.1139\n",
      "epoch: 7720, loss = 0.1135\n",
      "epoch: 7740, loss = 0.1131\n",
      "epoch: 7760, loss = 0.1127\n",
      "epoch: 7780, loss = 0.1122\n",
      "epoch: 7800, loss = 0.1118\n",
      "epoch: 7820, loss = 0.1114\n",
      "epoch: 7840, loss = 0.1110\n",
      "epoch: 7860, loss = 0.1106\n",
      "epoch: 7880, loss = 0.1102\n",
      "epoch: 7900, loss = 0.1097\n",
      "epoch: 7920, loss = 0.1093\n",
      "epoch: 7940, loss = 0.1089\n",
      "epoch: 7960, loss = 0.1086\n",
      "epoch: 7980, loss = 0.1082\n",
      "epoch: 8000, loss = 0.1078\n",
      "epoch: 8020, loss = 0.1074\n",
      "epoch: 8040, loss = 0.1070\n",
      "epoch: 8060, loss = 0.1066\n",
      "epoch: 8080, loss = 0.1062\n",
      "epoch: 8100, loss = 0.1059\n",
      "epoch: 8120, loss = 0.1055\n",
      "epoch: 8140, loss = 0.1051\n",
      "epoch: 8160, loss = 0.1048\n",
      "epoch: 8180, loss = 0.1044\n",
      "epoch: 8200, loss = 0.1040\n",
      "epoch: 8220, loss = 0.1037\n",
      "epoch: 8240, loss = 0.1033\n",
      "epoch: 8260, loss = 0.1030\n",
      "epoch: 8280, loss = 0.1026\n",
      "epoch: 8300, loss = 0.1023\n",
      "epoch: 8320, loss = 0.1020\n",
      "epoch: 8340, loss = 0.1016\n",
      "epoch: 8360, loss = 0.1013\n",
      "epoch: 8380, loss = 0.1009\n",
      "epoch: 8400, loss = 0.1006\n",
      "epoch: 8420, loss = 0.1003\n",
      "epoch: 8440, loss = 0.0999\n",
      "epoch: 8460, loss = 0.0996\n",
      "epoch: 8480, loss = 0.0993\n",
      "epoch: 8500, loss = 0.0990\n",
      "epoch: 8520, loss = 0.0987\n",
      "epoch: 8540, loss = 0.0983\n",
      "epoch: 8560, loss = 0.0980\n",
      "epoch: 8580, loss = 0.0977\n",
      "epoch: 8600, loss = 0.0974\n",
      "epoch: 8620, loss = 0.0971\n",
      "epoch: 8640, loss = 0.0968\n",
      "epoch: 8660, loss = 0.0965\n",
      "epoch: 8680, loss = 0.0962\n",
      "epoch: 8700, loss = 0.0959\n",
      "epoch: 8720, loss = 0.0956\n",
      "epoch: 8740, loss = 0.0953\n",
      "epoch: 8760, loss = 0.0950\n",
      "epoch: 8780, loss = 0.0947\n",
      "epoch: 8800, loss = 0.0944\n",
      "epoch: 8820, loss = 0.0941\n",
      "epoch: 8840, loss = 0.0939\n",
      "epoch: 8860, loss = 0.0936\n",
      "epoch: 8880, loss = 0.0933\n",
      "epoch: 8900, loss = 0.0930\n",
      "epoch: 8920, loss = 0.0927\n",
      "epoch: 8940, loss = 0.0925\n",
      "epoch: 8960, loss = 0.0922\n",
      "epoch: 8980, loss = 0.0919\n",
      "epoch: 9000, loss = 0.0917\n",
      "epoch: 9020, loss = 0.0914\n",
      "epoch: 9040, loss = 0.0911\n",
      "epoch: 9060, loss = 0.0909\n",
      "epoch: 9080, loss = 0.0906\n",
      "epoch: 9100, loss = 0.0903\n",
      "epoch: 9120, loss = 0.0901\n",
      "epoch: 9140, loss = 0.0898\n",
      "epoch: 9160, loss = 0.0896\n",
      "epoch: 9180, loss = 0.0893\n",
      "epoch: 9200, loss = 0.0891\n",
      "epoch: 9220, loss = 0.0888\n",
      "epoch: 9240, loss = 0.0886\n",
      "epoch: 9260, loss = 0.0883\n",
      "epoch: 9280, loss = 0.0881\n",
      "epoch: 9300, loss = 0.0878\n",
      "epoch: 9320, loss = 0.0876\n",
      "epoch: 9340, loss = 0.0873\n",
      "epoch: 9360, loss = 0.0871\n",
      "epoch: 9380, loss = 0.0868\n",
      "epoch: 9400, loss = 0.0866\n",
      "epoch: 9420, loss = 0.0864\n",
      "epoch: 9440, loss = 0.0861\n",
      "epoch: 9460, loss = 0.0859\n",
      "epoch: 9480, loss = 0.0857\n",
      "epoch: 9500, loss = 0.0854\n",
      "epoch: 9520, loss = 0.0852\n",
      "epoch: 9540, loss = 0.0850\n",
      "epoch: 9560, loss = 0.0848\n",
      "epoch: 9580, loss = 0.0845\n",
      "epoch: 9600, loss = 0.0843\n",
      "epoch: 9620, loss = 0.0841\n",
      "epoch: 9640, loss = 0.0839\n",
      "epoch: 9660, loss = 0.0836\n",
      "epoch: 9680, loss = 0.0834\n",
      "epoch: 9700, loss = 0.0832\n",
      "epoch: 9720, loss = 0.0830\n",
      "epoch: 9740, loss = 0.0828\n",
      "epoch: 9760, loss = 0.0826\n",
      "epoch: 9780, loss = 0.0824\n",
      "epoch: 9800, loss = 0.0821\n",
      "epoch: 9820, loss = 0.0819\n",
      "epoch: 9840, loss = 0.0817\n",
      "epoch: 9860, loss = 0.0815\n",
      "epoch: 9880, loss = 0.0813\n",
      "epoch: 9900, loss = 0.0811\n",
      "epoch: 9920, loss = 0.0809\n",
      "epoch: 9940, loss = 0.0807\n",
      "epoch: 9960, loss = 0.0805\n",
      "epoch: 9980, loss = 0.0803\n",
      "epoch: 10000, loss = 0.0801\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "# ============================ step 5/6 training ============================    \n",
    "for epoch in range(num_epochs):\n",
    "    models.train()\n",
    "    optimizers.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred  = models(X_train)\n",
    "    # Compute Loss\n",
    "    loss = criterions(y_pred, y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizers.step()\n",
    "    if (epoch+1) % 20 == 0:                                         \n",
    "        # printing loss values on every 10 epochs to keep track\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \n",
    "+ when you call `models(X_train)`, you automatically call `models.forward()` to propagate forward.\n",
    "+  Next, the loss is calculated. When `loss.backward()` is called, it computes the loss gradient with respect to the weights (of the layer). \n",
    "+ The weights are then updated by calling `optimizer.step()`. \n",
    "+ After this, the weights have to be emptied for the next iteration. So the `zero_grad()` method is called.\n",
    "\n",
    "The above code prints the loss at each 20th epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ step 7 Model Performance\n",
    "  \n",
    "Let us finally see the model accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = models(X_test)\n",
    "    y_pred = nn.Softmax(dim=1)(logits)\n",
    "    y_predicted_cls = y_pred.argmax(1)\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.96      0.96      0.96        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (50 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP(50 points )\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
